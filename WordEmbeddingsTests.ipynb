{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# First imports"
      ],
      "metadata": {
        "id": "GKy5aPZyp3QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install() # expect a kernel restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ8_mrHzhJQs",
        "outputId": "8fb0abdf-3674-4068-8a2f-35b560349c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "MSEJfRRqq1Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resource check"
      ],
      "metadata": {
        "id": "lml7A1uyQzOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "sMQQo_kCQ8Ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82896d91-7f4c-434b-fe76-b6b640a19251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "vFdgo4OgQ8-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install faiss"
      ],
      "metadata": {
        "id": "nZ2_VZAGrKYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c pytorch -c nvidia faiss-gpu=1.9.0"
      ],
      "metadata": {
        "id": "quq1bfmcyMGL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss"
      ],
      "metadata": {
        "id": "gT4Z5Kq8rUIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a sample of 800 words:\n",
        "\n",
        "# test_words = []\n",
        "# test_indices = np.random.randint(0, 10000, 800)\n",
        "# with open(\"/content/drive/MyDrive/wordlist.10000.txt\", 'r') as f:\n",
        "#   all_words = f.readlines()\n",
        "#   for idx in test_indices:\n",
        "#     test_words.append(all_words[idx].strip())\n",
        "\n",
        "# # save the test words to a file\n",
        "# with open(\"/content/drive/MyDrive/test_words\", 'w') as f:\n",
        "#   f.write(\",\".join(test_words))\n",
        "\n",
        "# print(test_words)\n"
      ],
      "metadata": {
        "id": "IvGhkHxysxyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Test Words"
      ],
      "metadata": {
        "id": "zJrWd0l7twL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the test  words\n",
        "test_words = []\n",
        "with open(\"/content/drive/MyDrive/test_words\", 'r') as f:\n",
        "  all_words = f.readlines()[0].split(\",\")[:-1] # removing the last empty string\n",
        "  for w in all_words:\n",
        "    test_words.append(w.strip())\n",
        "\n",
        "print(test_words)"
      ],
      "metadata": {
        "id": "tlM6IMiaRz6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test OpenAi and definitions - results bad"
      ],
      "metadata": {
        "id": "8tt_y_46qPC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "4MEmQpm_-Z_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "model = \"text-embedding-3-large\"\n",
        "dimensions = 3072\n",
        "words = test_words\n",
        "embeddings = np.zeros((len(words), dimensions))\n",
        "\n",
        "def get_embedding(text, model=model):\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n"
      ],
      "metadata": {
        "id": "7idt1EYSeFoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = tqdm(total=100)\n",
        "\n",
        "for idx, word in enumerate(words):\n",
        "  embeddings[idx] = get_embedding(word)\n",
        "  pbar.update(100/len(words))\n",
        "\n",
        "pbar.close()"
      ],
      "metadata": {
        "id": "TsdNk86NTs1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the test embeddings\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Data/test_embeddings.npy\", embeddings)\n"
      ],
      "metadata": {
        "id": "TnyIt-2YvSUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils etc"
      ],
      "metadata": {
        "id": "DcafkV3KZ8mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Build HNSW index\n",
        "def build_hnsw_index(embeddings, dimensions, M=32, efConstruction=100):\n",
        "  index = faiss.IndexHNSWFlat(dimensions, 32)  # M is a tunable parameter\n",
        "  index.hnsw.efConstruction = 100  # Higher efConstruction leads to better accuracy\n",
        "  index.add(embeddings)\n",
        "  return index\n",
        "\n"
      ],
      "metadata": {
        "id": "jw5Ha_eAZWV-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "52e80d7c-cdf4-4777-890b-c682e7cc9ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-15e728492b5e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Build HNSW index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_hnsw_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mefConstruction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexHNSWFlat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# M is a tunable parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_word_to_id = {word: idx for idx, word in enumerate(words)}"
      ],
      "metadata": {
        "id": "mEeE91tYq7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neighbour_words(query_word, k):\n",
        "  query_vector = embeddings[test_word_to_id[query_word]][None,:]\n",
        "  distances, indices = index.search(query_vector, k)\n",
        "  print(f\"distances : {distances}\")\n",
        "  print(f\"indices : {indices}\")\n",
        "\n",
        "  return [words[i] for i in indices[0]]\n",
        "\n",
        "def get_distance_between_words(word_1, word_2 ,similarity=\"cosine\"):\n",
        "  query_vector_1 = embeddings[test_word_to_id[word_1]]\n",
        "  query_vector_2 = embeddings[test_word_to_id[word_2]]\n",
        "  if similarity == \"cosine\":\n",
        "    return np.dot(query_vector_1, query_vector_2) / (np.linalg.norm(query_vector_1) * np.linalg.norm(query_vector_2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "toDpqbBCorgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "from io import TextIOWrapper\n",
        "\n",
        "\n",
        "def extract_and_load(file_path, number_of_rows):\n",
        "  with open(file_path, 'rb') as f:\n",
        "    df = pd.read_csv(f, nrows=number_of_rows)  # Load only first 500 rows\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_and_load_zipped(zip_path, number_of_rows):\n",
        "    # Open the zip file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        # Assuming the ZIP contains only one CSV file, get its name\n",
        "        csv_filename = z.namelist()[0]  # Get the first file name in the ZIP\n",
        "\n",
        "        # Open the CSV file inside the ZIP and read only the first 500 rows\n",
        "        with z.open(csv_filename) as csv_file:\n",
        "            df = pd.read_csv(TextIOWrapper(csv_file), nrows=number_of_rows)  # Load only first 500 rows\n",
        "\n",
        "            return df\n"
      ],
      "metadata": {
        "id": "SZT2RlTgbSep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_words)"
      ],
      "metadata": {
        "id": "BIhHqdjhrij9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_words = get_neighbour_words(\"payday\", 10)\n",
        "print(result_words)"
      ],
      "metadata": {
        "id": "gcy_qj82qrXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_distance_between_words(\"payday\", \"craps\"))"
      ],
      "metadata": {
        "id": "UOLuqgV_u1q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "query_word = \"policy\"\n",
        "\n",
        "query_vector = embeddings[test_word_to_id[query_word]]\n",
        "neighbour_indices = []\n",
        "similarities = np.dot(embeddings, query_vector) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_vector))\n",
        "\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "neighbour_words = test_words[sorted_indices][1:k+1]\n",
        "\n",
        "print(neighbour_words)\n"
      ],
      "metadata": {
        "id": "Ww7MDwPIz5JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test GloVe embeddings"
      ],
      "metadata": {
        "id": "CQvxrxgqTKe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load some example words"
      ],
      "metadata": {
        "id": "jsQ26X03eTNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import set_coroutine_origin_tracking_depth\n",
        "import numpy as np\n",
        "number_of_rows = 10000\n",
        "\n",
        "word_to_idx = {}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Data/glove.6B.300d.txt\", 'r') as f:\n",
        "\n",
        "  for i in range(number_of_rows):\n",
        "    word_to_idx[ f.readline().split()[0] ] = i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5SX_1dRyTTG7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Data/glove.6B.300d.txt\", 'r') as f:\n",
        "  eb = np.loadtxt(\n",
        "    f,\n",
        "    delimiter=' ',\n",
        "    skiprows=1,\n",
        "    usecols=range(1, 300),\n",
        "    max_rows=500\n",
        "  )"
      ],
      "metadata": {
        "id": "YZNqcwO8oMou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build an index"
      ],
      "metadata": {
        "id": "yHHVFSS2eqLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build HNSW index\n",
        "index = faiss.IndexHNSWFlat(dimensions, 32)  # M is a tunable parameter\n",
        "index.hnsw.efConstruction = 100  # Higher efConstruction leads to better accuracy\n",
        "index.add(eb)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FVPTCNRIhATy",
        "outputId": "5fa6d5e2-02b1-4e7e-fbb5-6fe1a82c838c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'faiss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-25d028f59993>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build HNSW index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexHNSWFlat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# M is a tunable parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhnsw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefConstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m  \u001b[0;31m# Higher efConstruction leads to better accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'faiss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "rl9kHBFzrjYn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTXOlyiSrm_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test conceptnet-numberbatch"
      ],
      "metadata": {
        "id": "XRYgXF0haG4D"
      }
    }
  ]
}